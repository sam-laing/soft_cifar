\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{peterson2019humanuncertaintymakesclassification}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{liu2020simpleprincipleduncertaintyestimation}
\citation{duq}
\citation{JMLR:v15:srivastava14a}
\citation{pmlr-v48-gal16}
\citation{zhang2018mixupempiricalriskminimization}
\citation{yun2019cutmixregularizationstrategytrain}
\citation{devries2017improvedregularizationconvolutionalneural}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}ResNet Modifications Used}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Metrics}{2}{subsection.2.2}\protected@file@percent }
\citation{guo2017calibrationmodernneuralnetworks}
\citation{goodfellow2015explainingharnessingadversarialexamples}
\citation{duq}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Outline}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Comparison of Uncertainty Methods}{4}{subsection.3.1}\protected@file@percent }
\citation{NEURIPS2019_9015}
\citation{peterson2019humanuncertaintymakesclassification}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{5}{section.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of different models.  For each of the 5 model types, we put the best metric in bold. The best metric across all models is highlighted in red.\relax }}{6}{table.caption.17}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:model_comparison}{{1}{6}{Comparison of different models.\\ For each of the 5 model types, we put the best metric in bold. The best metric across all models is highlighted in red.\relax }{table.caption.17}{}}
\citation{peterson2019humanuncertaintymakesclassification}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of Hard and Soft Label Reliability and Proportion Diagrams\relax }}{7}{figure.caption.21}\protected@file@percent }
\newlabel{fig:2x2grid}{{1}{7}{Comparison of Hard and Soft Label Reliability and Proportion Diagrams\relax }{figure.caption.21}{}}
\citation{anthony2023usemahalanobisdistanceoutofdistribution}
\citation{collier2023massivelyscalingheteroscedasticclassifiers}
\citation{vyas2020learningsoftlabelsmeta}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{anthony2023usemahalanobisdistanceoutofdistribution}{{1}{2023}{{Anthony and Kamnitsas}}{{}}}
\bibcite{collier2023massivelyscalingheteroscedasticclassifiers}{{2}{2023}{{Collier et~al.}}{{Collier, Jenatton, Mustafa, Houlsby, Berent, and Kokiopoulou}}}
\bibcite{devries2017improvedregularizationconvolutionalneural}{{3}{2017}{{DeVries and Taylor}}{{}}}
\bibcite{pmlr-v48-gal16}{{4}{2016}{{Gal and Ghahramani}}{{}}}
\bibcite{goodfellow2015explainingharnessingadversarialexamples}{{5}{2015}{{Goodfellow et~al.}}{{Goodfellow, Shlens, and Szegedy}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Potential Future Work}{8}{section.6}\protected@file@percent }
\bibcite{guo2017calibrationmodernneuralnetworks}{{6}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{liu2020simpleprincipleduncertaintyestimation}{{7}{2020}{{Liu et~al.}}{{Liu, Lin, Padhy, Tran, Bedrax-Weiss, and Lakshminarayanan}}}
\bibcite{NEURIPS2019_9015}{{8}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{peterson2019humanuncertaintymakesclassification}{{9}{2019}{{Peterson et~al.}}{{Peterson, Battleday, Griffiths, and Russakovsky}}}
\bibcite{JMLR:v15:srivastava14a}{{10}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{duq}{{11}{2020}{{van Amersfoort et~al.}}{{van Amersfoort, Smith, Teh, and Gal}}}
\bibcite{vyas2020learningsoftlabelsmeta}{{12}{2020}{{Vyas et~al.}}{{Vyas, Saxena, and Voice}}}
\bibcite{yun2019cutmixregularizationstrategytrain}{{13}{2019}{{Yun et~al.}}{{Yun, Han, Oh, Chun, Choe, and Yoo}}}
\bibcite{zhang2018mixupempiricalriskminimization}{{14}{2018}{{Zhang et~al.}}{{Zhang, Cisse, Dauphin, and Lopez-Paz}}}
\gdef \@abspage@last{9}
